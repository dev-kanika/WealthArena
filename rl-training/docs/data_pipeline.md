# Data Pipeline

The data platform follows a medallion architecture that captures raw market activity, enforces governance, and serves feature-rich datasets to the reinforcement learning stack.

## Medallion Layers

- **Bronze (Raw Ingestion)**: Partitioned Parquet files under `data/bronze/<asset_class>/<source>/<symbol>/<YYYY>/<MM>/<DD>.parquet`. Files are immutable snapshots generated by collectors with JSON metadata (source, identifier, timestamps, row count). Partitioning is by acquisition date and data source to keep incremental loads efficient.
- **Silver (Validated Schema)**: Cleaned tables with canonical schema: `timestamp_utc`, `symbol`, OHLCV columns, and `metadata` (dict of provider-specific attributes). `BronzeToSilverProcessor` normalizes timezone (UTC), fills gaps, applies corporate actions, and persists quality flags (`is_holiday`, `is_synthesized`, `missing_bars`).
- **Gold (Feature Store)**: Feature-aligned datasets ready for RL training. Pipelines compute:
  - Technical indicators with TA-Lib fallbacks (SMAs, EMAs, RSI, MACD, ATR, Bollinger bands).
  - Sentiment features from FinBERT (news) and BERTweet (social), embedded with windowed aggregations and attention scores.
  - Macro transforms and surprises (diffs, z-scores, growth rates).
  - Covariance estimates using Ledoit–Wolf shrinkage plus regime-aware adjustments.

## Collection Entrypoints

- `scripts/collect_all_data.py --config config/data_config.yaml` orchestrates multi-source ingestion (Yahoo Finance, CCXT, Alpha Vantage, FRED, Reddit/Twitter) and persists to the Bronze layer.
- Asset-class-specific scripts (`scripts/collect_stocks.py`, `scripts/collect_forex.py`, `scripts/collect_news.py`, etc.) inherit the same configuration but allow ad hoc refreshes.
- Collectors (`src/data/collectors.py`) enforce rate limits (`RateLimiter`), retries, and metadata preservation consistent with the logging utilities in `src/utils`.

## Stock Catalog Generation

- **Purpose**: Produce sector-scoped stock universes that feed the Bronze collectors. Without catalog files `collect_stocks.py` falls back to a minimal safety list; generating catalogs ensures 50 symbols per sector as defined in `config/data_config.yaml`.
- **Yahoo Finance Screener API**: Uses `https://query2.finance.yahoo.com/v1/finance/screener` with POST payloads shaped as nested operators (`and`/`or`). Requests pull up to 250 symbols per page, iterating with `offset` until exhausted. Apply 0.3–0.5 second sleeps between calls and include a modern `User-Agent` header to avoid HTTP 403/429 responses.
- **Sector Mapping**:

  | Config Sector             | Yahoo Sector(s)                         | Representative Industries                                |
  |---------------------------|-----------------------------------------|----------------------------------------------------------|
  | `technology_services`     | Technology                              | Information Technology Services, Software (Infrastructure, Application) |
  | `electronic_technology`   | Technology                              | Semiconductors, Computer Hardware, Communication Equipment |
  | `finance`                 | Financial Services                      | Banks (Diversified), Capital Markets                     |
  | `health_technology`       | Healthcare                              | Healthcare Plans, Biotechnology, Medical Devices         |
  | `energy_minerals`         | Energy                                  | Oil & Gas E&P, Oil & Gas Integrated, Coal                |
  | `consumer`                | Consumer Defensive, Consumer Cyclical   | Specialty Retail, Consumer Electronics, Beverages (Non-Alcoholic) |
  | `industrial`              | Industrials                             | Aerospace & Defense, Specialty Industrial Machinery, Conglomerates |

- **Random Sampling**: The generator gathers the full Screener result set, deduplicates symbols, and randomly samples the sector `target_count` (default 50, configurable via CLI). If fewer than the requested count exist, all available symbols are written and a warning is logged.
- **Output Format**: CSV files with a single `symbol` column stored under `data/catalog/stocks/`, following the `universe_reference` paths declared in `config/data_config.yaml`.
- **Usage Examples**:
  - Baseline: `python scripts/generate_stock_catalog.py --config config/data_config.yaml`
  - Custom count: `python scripts/generate_stock_catalog.py --config config/data_config.yaml --target-count 100`
  - Reproducible sampling: `python scripts/generate_stock_catalog.py --config config/data_config.yaml --seed 42`
- **Troubleshooting**:
  - HTTP 403: Ensure the default User-Agent header is present or override with a modern browser string.
  - HTTP 429: Increase the inter-request delay (edit the script defaults if needed) or rerun after a short pause.
  - Empty results: Verify the sector key exists in the mapping and matches Yahoo Screener spelling; rerun after adjusting configuration.

## Processing Workflows

1. **Bronze → Silver**: Run `python scripts/process_bronze_to_silver.py --config config/data_config.yaml`. This stage:
   - Applies schema contracts and type coercion.
   - Aligns timestamps, forward-fills short gaps, and records validation flags in `data/audit/`.
   - Writes partitioned Silver outputs to `data/silver/<asset_class>/<symbol>/...`.
2. **Silver → Gold**: Execute `python scripts/process_silver_to_gold.py --feature-config config/feature_config.yaml`. The processor:
   - Merges Silver OHLCV, sentiment frames, macro series, and correlation tiles.
   - Derives feature matrices with TA-Lib indicators, FinBERT/BERTweet scores, macro shocks, and Ledoit–Wolf covariances.
   - Outputs Gold datasets to `data/gold/<feature_group>/...`.

For fully automated runs, launch `python scripts/process_data_pipeline.py` which chains ingestion, validation, and feature creation with consistent logging and exception handling.

## Quality & Validation

- `scripts/validate_data.py --layer gold` validates schema, completeness, and anomaly heuristics before agents reload data.
- Audit logs and validation reports are stored under `data/audit/<timestamp>/` for reproducibility.
- Checksums and schema versions are promoted alongside datasets to detect drift before training/serving.

## Configuration Surface

- **Data collection**: `config/data_config.yaml` defines symbols, lookback horizons, rate limits, retry budgets, and paths.
- **Feature engineering**: `config/feature_config.yaml` controls indicator windows, sentiment aggregation intervals, macro transformations, and covariance settings.
- **Environment**: `.env` or `.env.template` capture API keys (ALPHAVANTAGE_API_KEY, FRED_API_KEY, PRAW_*), caching toggles, and storage roots.

Follow the logger patterns in `src/utils.get_logger` when extending collectors or processors to ensure structured output and unified observability.
